input {

  #filebeat input
  beats {
    port            => 5044
    tags            => [ "filebeat" ]
    ssl_certificate => "{{logstash_dir}}/certs/{{logstash_certnamefb}}.cert.pem"
    ssl_key         => "{{logstash_dir}}/certs/{{logstash_certnamefb}}.key.pem"
    ssl             => true
  }

} #end input plugins
##########
#   Name:       Logstash Parser
#   Version:    1.2
#
#   1.) Check for application data
#   2.) Check for system/service data
#   3.) Check for other unrecognized data
#
# {{ ansible_managed }}
# Logstash Parser
#   - Pickup from Redis
#   - Reformat JSON to datastream
#   - Parse and output to Elasticsearch



# Remaining todo
# Set the date format for all the calls
# Check all calls for nessus activity
#

filter {

if "metricsets" not in [type] {
# Remaining todo
# Set the date format for all the calls
# Check all calls for nessus activity
#

#####################################
#   General, applies to all calls
##################################

   mutate {
     add_field => [ "{{ ansible_hostname }}", "%{@timestamp}" ]
     add_field => { "[@metadata][topic]" => "log.sys.log" }
   }

#   mutate {
#     add_field => { "siem_program" => "NULL" }
#   }

  if "all.json" not in [source] or "error_debug.json" or "burger_audit.json" not in [source] { ### mijnoverheid app logs are json based and do not need custom parsing
     mutate {
       add_field => { "filterset" => "syslog" }
     }


    if "/var/log/audit" in [source] {
      grok {
        match => [ "message", "%{SYSLOGTIMESTAMP:syslog_timestamp}%{SPACE}%{SYSLOGHOST}%{SPACE}%{PROG:program}(?:\[%{POSINT:pid}\])?:%{SPACE}%{GREEDYDATA:message}" ]
        add_tag => [ "%{program}" ]
        tag_on_failure => [ "_grokparsefailure" ]
      }
      mutate {
        add_field => { "[@metadata][topic]" => "adt.sys.log" }
      }
    } else {
      grok {
        match => [ "message", "%{SYSLOGTIMESTAMP:syslog_timestamp}%{SPACE}%{SYSLOGHOST}%{SPACE}%{PROG:program}(?:\[%{POSINT:pid}\])?:%{SPACE}%{GREEDYDATA:message}" ]
        add_tag => [ "%{program}" ]
        overwrite => [ "message" ]
        tag_on_failure => [ "_grokparsefailure" ]
      }
    }
    #algemene date verwerking behalve voor MO type regels die al de timestamp bevatten
    date {
      locale => "en"
      match => [ "syslog_timestamp" , "dd/MMM/YYYY:HH:mm:ss Z" , "MMM dd HH:mm:ss", "MMM  d HH:mm:ss" ]
      #match => [ "syslog_timestamp", "ISO8601" ]
    }
 }  #end if not mijnoverheid app logs
 #################################
 #   Name:       Logstash Parser
 #   Version:    1.1
 #
 #   1.) Check for application data
 #   2.) Check for system/service data
 #   3.) Check for other unrecognized data
 #
 # {{ ansible_managed }}
 # Logstash Parser
 #   - Pickup from Redis
 #   - Reformat JSON to datastream
 #   - Parse and output to Elasticsearch
 #
 # In this file: Check for Application data (mo3)
 
 
 # Remaining todo
 # Set the date format for all the calls
 # Check all calls for nessus activity
 #
 
 #####################################################################################
 #   Application      Application data    Application data    Application data    #
 #####################################################################################
 
 #HIER ALLES IN WAT MO3 IS
   if "mo3" in [tags] or "mo3-audit" in [tags] {
     #mijnoverheid3 specific options
     #mo3 stuurt alleen json files in json formaat - log extentie wordt nog even genegeerd
 
     # Create basic data format
     mutate {
       add_field   => { "filterset" => "mo3" }
       add_tag     => ["mo3_v1"]
     }
 
     json_encode {
       source => "exception"
       target => "exception_message"
       remove_field => [ "exception" ]
     }
 
     json_encode {
       source => "result"
       target => "result_message"
       remove_field => [ "result" ]
     }
 
 
     # Toevoeging om data te anonimeseren zodat developers toegang kunnen krijgen tot deze data.
     if [bsn] {
       fingerprint {
         source => "bsn"
         target => "bsn_hash"
         method => "MURMUR3"
       }
     }
 
     if [client_ip] {
       fingerprint {
         source => "client_ip"
         target => "client_ip_hash"
         method => "MURMUR3"
       }
     }
 
     if [request_ip] {
       fingerprint {
         source => "request_ip"
         target => "request_ip_hash"
         method => "MURMUR3"
       }
     }
 
     if [anummer] {
       fingerprint {
         source => "anummer"
         target => "anummer_hash"
         method => "MURMUR3"
       }
     }
 
     if "belanghebbende" in [url] {
       grok {
         match        => [ "url", "/%{DATA:part1}/%{DATA:part2}/%{DATA:part3}/%{DATA:part4}/%{GREEDYDATA:part_rest}" ]
         add_field    => { "url_dev" => "/%{part1}/%{part2}/burger/%{part4}/" }
         remove_field => [ "%{part1}, %{part2}, %{part3}, %{part4}, %{part_rest}" ]
       }
     }
 
     if "rdw/detail" in [url]{
         grok {
         match        => [ "url", "/%{DATA:part1}/%{DATA:part2}/%{DATA:part3}/%{DATA:part4}/%{GREEDYDATA:part_rest}" ]
         add_field    => { "url_dev" => "/%{part1}/%{part2}/kenteken/%{part_rest}" }
         remove_field => [ "%{part1}, %{part2}, %{part3}, %{part4}, %{part_rest}" ]
       }
     }
 
 
     if "woz" in [url] {
       if "bag" in [url] {
         grok {
           match        => [ "url", "/%{DATA:part1}/%{DATA:part2}/%{DATA:part3}/%{DATA:part4}/%{GREEDYDATA:part_rest}" ]
           add_field    => { "url_dev" => "/%{part1}/%{part2}/code/burger/%{part_rest}" }
           remove_field => [ "%{part1}, %{part2}, %{part3}, %{part4}, %{part_rest}" ]
         }
       } else {
         if "detail" in [url] {
           grok {
             match        => [ "url", "/%{DATA:part1}/%{DATA:part2}/%{DATA:part3}/%{DATA:part4}/%{DATA:part5}\?%{GREEDYDATA:part_rest}" ]
             add_field    => { "url_dev" => "/%{part1}/%{part2}/%{part3}/%{part4}/code/%{part_rest}" }
             remove_field => [ "%{part1}, %{part2}, %{part3}, %{part4}, %{part_rest}" ]
           }
         } else {
           grok {
             match        => [ "url", "/%{DATA:part1}/%{DATA:part2}/%{DATA:part3}/%{GREEDYDATA:part_rest}" ]
             add_field    => { "url_dev" => "/%{part1}/%{part2}/code/burger/" }
             remove_field => [ "%{part1}, %{part2}, %{part3}, %{part4}, %{part_rest}" ]
           }
         }
       }
     }
 
     if ![url_dev] {
       mutate { copy => { "url" => "url_dev" }}
     }
 
 
   } # endif mo3 in tag
   #################################
   #   Name:       Logstash Parser
   #   Version:    2.0
   #
   #   1.) Check for application data
   #   2.) Check for system/service data
   #   3.) Check for other unrecognized data
   #
   # {{ ansible_managed }}
   
   
   #####################################
   #   General, applies to all calls
   ##################################
   # Applicable to all log lines except metricbeat data, which has a type of metricsets
      if "filebeat" in [tags] {
         mutate {
            add_tag     => { "parser_to-redis_script_version" => "2.0" }
             rename => { "offset" => "offset_fb" }
             add_field => [ "offset" , "%{host} - %{offset_fb}" ]
         }
   
         date {
           locale => "en"
           # voor MO is de syslog_timestamp al de juiste match - testen en opschonen waar mogelijk
           match => [ "syslog_timestamp" , "dd/MMM/YYYY:HH:mm:ss Z" , "MMM dd HH:mm:ss", "MMM  d HH:mm:ss" ]
         }
   
         mutate {
            add_field => [ "received_at", "%{@timestamp}" ]
            add_field => [ "{{ ansible_hostname }}", "%{@timestamp}" ]
         }
   
         if ".json" in [source] {
         # om te parsen moet de door MO aangeleverde json regel naar een data structuur worden omgezet
         # dat doet het onderstaande filter
            json {
               source => "message"
            }
   
            mutate {
               add_tag => [ "json" ]
            }
   
         } #end if error_debug.json in source or all.json in source
         # eerste stap naar immutability
         fingerprint {
            id => "idempotency_fp"
            source => ["offset", "source"]
            key => "unieke_8753_WaARDE%!"
            concatenate_sources => true
            target => "generated_id"
            method => "SHA256"
         }
      } # end if filebeat in tag
   
     if "/var/log/secure" in [source] {
        mutate {
          add_tag => [ "to_siem" ]
        }
      }
      # Logstash Parser

      # Algemeen
      
      if "to_siem" in [tags] {
        if ![severity] {
          mutate { add_field => { "severity" => "Informational" } }
        }
        if ![siem_program] {
          mutate { copy => { "program" => "siem_program" }}
        }
      }
      
      
      #Close filter
      
        } # end filter plugins
      } # end if not type =  metricsets
      output {
        kafka {
          codec => json
          topic_id => "%{[@metadata][topic]}"
          acks => all
          bootstrap_servers => ["ops41-logas1:9092","ops41-logas2:9092",ops41-logas3:9092"]
          compression_type => "snappy"
        }
      }
            
        
 

